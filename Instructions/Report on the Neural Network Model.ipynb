{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93fd058-8a05-4b62-b115-396eefab2420",
   "metadata": {},
   "source": [
    "### Step 4: Write a Report on the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafb78a-c689-4024-8a2b-672278741387",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "Our report was compiled together to assess the applicants for funding on ventures. The model judges the applicants based on different variables such as the featured \"IS_SUCCESSFUL\" variable. This variable was used to predict whether or not the applicant would be successful. \n",
    "\n",
    "Other variables assessed with this model to tune the results and accuracy were APPLICATION_TYPE, AFFILIATION, CLASSIFICATION, USE_CASE, ORGANIZATION, STATUS, INCOME_AMT, SPECIAL_CONSIDERATIONS,and ASK_AMT.\n",
    "\n",
    "The variables that I had removed by the instructions of the project were \"EIN\" and \"NAME\".\n",
    "\n",
    "**Results**\n",
    "The initial model that was in the starter code had 2 hidden layers. One layer had 80 and the other had 30 neurons. The activation for these layers was ReLu but the output layer used the sigmoid activation. \n",
    "\n",
    "For the optimization, I started to add additional layers, but it seemed to lead me further from the accuracy desired. I discussed with my colleague B Slone (https://github.com/bslone1/deep-learning-challenge) and they told me that I should start smaller in what I should adjust. I added an additional layer from the starter code, but made the activation be sigmoid in order to improve the model's accuracy.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The model that was created started with the starter code had a accuracy of just over 73%. There was a concern for creating a vansihing gradient problem by using two sigmoid activations. This would have led to compressing the input through the sigmoid and not created a difference in the output. So, when I tried to use additional hidden layers, it made sense why there was not a big change in the output. The sigmoid was going to compress that output and continually keep it around 73-75% accuracy. After having discussed the changes made with my colleage, B Slone, we thought it would be usuable to have the ReLu function reduce the vanishing gradient issue and making the analysis valid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0f72f-0f16-43f0-8824-4c53627c9207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
